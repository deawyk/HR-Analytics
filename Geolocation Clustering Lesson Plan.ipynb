{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guest Instructor - HR Analytics\n",
    "\n",
    "### Day 1: Data gathering & ETL (Excel, Tableau)\n",
    "Intro to HR Analytics\n",
    "- Common Names: People Analytics, Talent Analytics, People Insights\n",
    "- Organization Structure and Functions\n",
    "- Business Areas\n",
    "    - Talent Acquisition\n",
    "    - HR Management\n",
    "    - Operational Effectiveness\n",
    "    - Diversity & Inclusion\n",
    "    - Learning & Development\n",
    "    - Performance Evaluation, Compensation etc.\n",
    "   \n",
    "Webscraping using Excel\n",
    "\n",
    "Gathering & exporting city coordinates from Tableau\n",
    "\n",
    "ETL in Python (S. 1)\n",
    "\n",
    "Files: ```canadacities.csv```\n",
    "\n",
    "### Day 2: Clustering Algorithms (Python, Jupyter Notebook)\n",
    "Working in HR Analytics\n",
    "- Vendor Tools\n",
    "    - LinkedInn Insights\n",
    "    - Glassdoor, Indeed\n",
    "    - Gartner, Talent Neuron\n",
    "- Popular Projects\n",
    "    - Sentiment Analysis\n",
    "    - Location Strategy\n",
    "    - Skills Mapping\n",
    "    - Career Pathing\n",
    "    - Surveying, Email Nudging, etc.\n",
    "\n",
    "Clustering Algorithms in Python (S. 2-5)\n",
    "\n",
    "Take home: Do the same for US cities ```uscities.csv```\n",
    "\n",
    "Can you create a function that will cluster according to your needs at the click of a button?\n",
    "\n",
    "### Day 3: Reporting and Driving Insights (Tableau)\n",
    "Career Advice\n",
    "- Interview Process and Prepping\n",
    "- Working Style: High-Level vs. Detailed, Process-oriented vs. Results-oriented\n",
    "- Mentorship: WeCareer\n",
    "    \n",
    "Create Final Report in Tableau\n",
    "- Calculated Field: Number of Cities in Cluster\n",
    "- Filters: City Type, Population\n",
    "- Parameters: Include Outliers T|F\n",
    "- Formatting: Aliases, Colors, Shape and size\n",
    "    \n",
    "Take home: Replicate completed report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hdbscan\n",
    "!pip install folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python Packages\n",
    "<b>HDBSCN</b>: https://hdbscan.readthedocs.io/en/latest/index.html<br>\n",
    "<b>Folium</b>: https://python-visualization.github.io/folium/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import folium\n",
    "import re\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formart = 'svg'\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Preparing the Data\n",
    "#### Load, Clean, Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../Data/canadacities.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated()\n",
    "# df.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Drop cities based on population threshold\n",
    "df_drop1 = df.drop(df[df.population_2020 < 50000].index)\n",
    "\n",
    "# Option 2: Drop cities based on city type\n",
    "df_drop2 = df.drop(df[df.type == 'CA'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract coordinates for faster computation\n",
    "X = np.array(df[['lat','long']], dtype='float64')\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Assign weights to cities based on size (Core Predict)\n",
    "\n",
    "##### Option 3. Using Population\n",
    "```weight = ln(population) - q```\n",
    "- As the population approaches its carrying capacity due to limited resources, it exhibits logarithmic growth\n",
    "- q is a Qualifier that drops cities if they do not meet population threshold\n",
    "\n",
    "##### Option 4. Using Class Ranking\n",
    "```weight = 1 / ranking * 5```\n",
    "- For some datasets, cities are ranked 1-4 based on their level i.e. capital city, metropolitan, town, county etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3\n",
    "df['weight'] = (np.log(df.population_2020) - 8).astype('int')\n",
    "\n",
    "# Option 4\n",
    "df['weight'] = (1 / df.ranking*5).astype('int')\n",
    "\n",
    "# duplicate rows by weight\n",
    "df_weight = df.reindex(df.index.repeat(df.weight)).reset_index(drop=True)\n",
    "\n",
    "# carry out the clustering using df_weight, Step 5 will drops duplicates before saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Validating the Data\n",
    "#### Visualizations with Folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic coordinates distribution\n",
    "plt.scatter(X[:,0], [X[:,1], alpha=0.2, s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geolocation mapping\n",
    "m = folium.Map(location=[df.lat.mean(), df.long.mean()], zoom_start=9, tiles='Stamen Toner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.iterrows():\n",
    "    folium.CircleMarker(location=[row.lat, row.long]).add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting - regex match all strings that contain a non-letter\n",
    "for _, row in df.iterrows():\n",
    "    folium.CircleMarker(location=[row.lat, row.long],\n",
    "                        radius=5,\n",
    "                        popup=re.sub(r'[^a-zA-Z ]+', '', row.city),\n",
    "                        color='#1787FE',\n",
    "                        fill=True,\n",
    "                        fill_color='#1787FE'\n",
    "                       ).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_map(df, cluster_column):\n",
    "    m = folium.Map(location=[df.lat.mean(), df.long.mean()], zoom_start=9, tiles='Stamen Toner')\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if row[cluster_column] == -1:\n",
    "            cluster_color = '#000000'\n",
    "        else:\n",
    "            cluster_color = cols[row[cluster_column]]\n",
    "            \n",
    "        folium.CircleMarker(location=[row['lat'], row['long']],\n",
    "                            radius=5,\n",
    "                            popup=row[cluster_column],\n",
    "                            color=cluster_color,\n",
    "                            fill=True,\n",
    "                            fill_color=cluster_color\n",
    "                           ).add_to(m)\n",
    "        \n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Exploring Solutions\n",
    "### Solution A. DBSCAN\n",
    "Density-Based Spatial Clustering of Applications with Noise<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "\n",
    "#### Parameters\n",
    "```eps``` = episolon is the local radius of expanding clusters\n",
    "- DBSCAN never takes a step larger than eps, but by doing multiple steps cluster can become much bigger than eps\n",
    "- radian to kilometer coversion: eps = x/6371\n",
    "- radian to miles conversion: eps = x/3959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \n",
    "model1 = DBSCAN(eps=x/6371, min_samples=2, algorithm='ball_tree', metric='haversine').fit(np.radians(X))\n",
    "class_predictions = model1.labels_\n",
    "\n",
    "class_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CLUSTER_DBSCAN'] = class_predictions\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of clusters: {len(np.unique(class_predictions))}')\n",
    "print(f'Number of cutliers: {len(class_preedictions[class_predictions==-1])}')\n",
    "print(f'Silhouette score: {silhouette_score(X[class_predictions!=-1], class_predictions[class_predictions!=-1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = create_map(df, 'CLUSTER_DBSCAN')\n",
    "\n",
    "m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution B. HDBSCAN\n",
    "Hierarchical DBSCAN allows varying density of clusters<br>\n",
    "https://hdbscan.readthedocs.io/en/latest/parameter_selection.html\n",
    "\n",
    "#### Parameters\n",
    "```min_samples``` = minimum number of neighbours to a core point<br>\n",
    "```min_cluster_size``` = minimum size a final cluster can be\n",
    "\n",
    "- Increasing min_samples will increase the size of the clusters, but it does so by discarding data as outliers via DBSCAN\n",
    "- Increasing min_cluster_size while keeping min_samples small keeps those outliers and merges any small clusters with their most similar neighbor until all clusters are above min_cluster_size\n",
    "- This is the H part of HDBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 40\n",
    "y = 5\n",
    "z = 2\n",
    "model2 = hdbscan.HDBSCAN(min_cluster_size=y, min_samples=z, cluster_selection_epsilon=x/6371).fit(np.radians(X))\n",
    "class_predictions = model2.labels_\n",
    "\n",
    "df['CLUSTER_HDBSCAN'] = class_predictions\n",
    "\n",
    "print(f'Number of clusters: {len(np.unique(class_predictions))}')\n",
    "print(f'Number of cutliers: {len(class_preedictions[class_predictions==-1])}')\n",
    "print(f'Silhouette score: {silhouette_score(X[class_predictions!=-1], class_predictions[class_predictions!=-1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = create_map(df, 'CLUSTER_HDBSCAN')\n",
    "\n",
    "m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution C. Hybrid (HDBSCAN + K-Means Clustering)\n",
    "To eliminate outliers, two-step hybrid method groups them into pre-existing clusters using K-means algorithm.\n",
    "https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate, split, train\n",
    "classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "df_train = df[df.CLUSTER_HDBSCAN!=-1]\n",
    "df_predict = df[df.CLUSTER_HDBSCAN==-1]\n",
    "\n",
    "X_train = np.array(df_train[['lat', 'long']], dtype='float64')\n",
    "y_train = np.array(df_train['CLUSTER_HDBSCAN'])\n",
    "X_predict = np.array(df_predict[['lat', 'long']], dtype='float64')\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending cluster_hybrid column\n",
    "df['CLUSTER_HYBRID'] = df['CLUSTER_HDBSCAN']\n",
    "df.loc[df.CLUSTER_HDBSCAN==-1, 'CLUSTER_HYBRID'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = create_map(df, 'CLUSTER_HYBRID')\n",
    "\n",
    "m3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Comparing Solutions\n",
    "\n",
    "#### Using a simple histogram to compare and determine the optimal solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CLUSTER_DBSCAN'].value_counts().plot.hist(bins=70m alpha=0.4, label='DBSCAN')\n",
    "df['CLUSTER_HDBSCAN'].value_counts().plot.hist(bins=70m alpha=0.4, label='HDBSCAN')\n",
    "df['CLUSTER_HYBRID'].value_counts().plot.hist(bins=70m alpha=0.4, label='Hybrid')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Comparing DBSCAN, DBSCAN, and Hybrid Approaches')\n",
    "plt.xlabel('Cluster Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Save Data to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#organize\n",
    "df = df.drop_duplicates().sort_values(by=['CLUSTER_HDBSCAN', 'city'])\n",
    "#save\n",
    "df.to_csv('canadacities_CLUSTER.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension. Create a Function to run all the clustering algorithms with One-click\n",
    "What inputs and outputs does the function take?\n",
    "\n",
    "What are the parameters that the users can decide on?\n",
    "\n",
    "How to locate, load, and save data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint\n",
    "\n",
    "import os\n",
    "\n",
    "def city_cluster(files, eps, min_cl, min_sp):\n",
    "    for file in files:\n",
    "        df = pd.read_csv(f'Data/{file}')\n",
    "        X = np.array()\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "    print(os.listdir('Data'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
